# Pipeline d'extraction d'informations dans les documents

Pipeline pour Ã©tiqueter vos documents (informations clÃ©s Ã  extraire), augmenter vos donnÃ©es, affiner LayoutLMv3 dessus, effectuer des infÃ©rences sur de nouvelles donnÃ©es et visualiser les informations clÃ©s extraites ! Pas besoin de GPU pour entraÃ®ner et faire tourner le modÃ¨le final en infÃ©rence.

- ğŸ“„ **Peu de donnÃ©es nÃ©cessaires** : seulement une dizaine de documents Ã  labÃ©liser pour de bonnes performances.  
- ğŸ¨ **Interface de labÃ©lisation intuitive** : crÃ©ez tous les labels dont vous avez besoin, libre Ã  vous de dÃ©finir prÃ©cisÃ©ment les informations clÃ©s Ã  extraire.  
- ğŸ” **OCR ultra-performant** : dÃ©tection â€œintelligenteâ€ des rÃ©gions par gradient de Sobel pour un dÃ©coupage optimal et une reconnaissance de texte de haute qualitÃ©.  
- ğŸ”„ **Data augmentation** : gÃ©nÃ©rez automatiquement des variantes labÃ©lisÃ©es pour enrichir votre jeu de donnÃ©es et renforcer la robustesse du modÃ¨le.  
- ğŸ¤– **Fine-tuning dâ€™un Transformer multimodal lÃ©ger** : adaptez facilement [layoutlmv3-base](https://huggingface.co/microsoft/layoutlmv3-base) de Microsoft Ã  vos documents, sans GPU nÃ©cessaire.  
- ğŸ–¥ï¸ **Interface dâ€™infÃ©rence modulable** : visualisez directement les informations extraites, ou personnalisez librement le script `inference.py` pour exploiter vos rÃ©sultats selon vos besoins. Encore une fois, pas de GPU nÃ©cessaire pour l'infÃ©rence! Utiliser le script simplement en local!


## Lancement de l'outil
```shell
make all
```
Si tu as un nom spÃ©cifique pour ton projet:
```shell
make NAME=mon_projet all
```
Si tu ne veux lancer que sur l'infÃ©rence car tu as dÃ©jÃ  un modÃ¨le fine-tunÃ©
```shell
make NAME=mon_projet final_inference
```

### Workflow de lâ€™outil
```mermaid
flowchart TD
  A["annotate.py"] --> B["layoutlmv3_ft.py"]
  B --> C["inference.py"]
```


## Introduction de la tÃ¢che KIE
Key Information Extraction (KIE) consiste Ã  dÃ©tecter et Ã  extraire automatiquement des Ã©lÃ©ments structurÃ©s (champs-clÃ©s, entitÃ©s, paires clÃ©-valeur) Ã  partir de documents variÃ©s (formulaires, factures, reÃ§us, etc.). Il s'agit en fait d'une tÃ¢che de classification multi-classes des mots issus de l'OCR. 

### Jeux de donnÃ©es et exemples de tÃ¢ches
- **FUNSD** (Form Understanding in Noisy Scanned Documents) : extraction de paires clÃ©-valeur depuis des formulaires annotÃ©s avec positions de tokens et catÃ©gories sÃ©mantiques.
- **SROIE** (Scanned Receipt OCR and Information Extraction) : identification et classification des champs clÃ©s (nom du magasin, total, TVA, date) sur des reÃ§us de caisse.
- **CORD** (Complex Receipt Datasets) : version dÃ©taillÃ©e de reÃ§us permettant lâ€™extraction dâ€™informations plus diversifiÃ©es et la reconnaissance de tables.

Chaque dataset propose une **tÃ¢che de classification** (types de champs) et de **localisation** (boÃ®tes englobantes), ou une **tÃ¢che gÃ©nÃ©rative** (gÃ©nÃ©rer directement le JSON de sortie).

Ici, je propose une tÃ¢che de KIE "custom", avec les documents & labels au choix!

### Familles de modÃ¨les pour la KIE
Deux grandes catÃ©gories de modÃ¨les sâ€™affrontent sur ces tÃ¢ches :

#### 1. ModÃ¨les fine-tunÃ©s (add heads)
- **Principe** : partir dâ€™un backbone prÃ©-entraÃ®nÃ© (LayoutLMv3, Donut, etc.), ajouter une tÃªte spÃ©cialisÃ©e (classification, token classification) et fine-tuner sur la tÃ¢che cible.
- **Atouts** : lÃ©gers, rapides Ã  entraÃ®ner (quelques heures sur CPU ou petite GPU), nÃ©cessitent peu de ressources matÃ©rielles.
- **Exemples** :
  - **LayoutLMv3** : modÃ¨le multimodal traitant conjointement les tokens textuels, la mise en page (bboxes) et lâ€™information visuelle extraite via une architecture Transformer unifiÃ©e.
  - **LILT (TILT)** : extension de LayoutLM pour la gÃ©nÃ©ration de sorties structurÃ©es Ã  partir de tokens visuels et textuels, souvent utilisÃ©e en mode discriminatif.
- **Usage typique** : classification de tokens, extraction de paires clÃ©-valeur via softmax sur chaque token.

#### 2. ModÃ¨les gÃ©nÃ©ratifs (VLLMs)
- **Principe** : modÃ¨les de type Â« Vision + Language Large Models Â» qui reÃ§oivent en entrÃ©e lâ€™image du document et gÃ©nÃ¨rent sÃ©quentiellement le JSON ou la liste des champs.
- **Atouts** : flexibles, peuvent gÃ©rer des sorties hÃ©tÃ©rogÃ¨nes et imiter un assistant linguistique pour la documentation.
- **Exemple** :
  - **GenKIE** : gÃ©nÃ¨re directement les structures de sortie, robuste aux erreurs OCR.

> *Pour une revue dÃ©taillÃ©e des diffÃ©rentes familles de modÃ¨les KIE, voir le papier* [arXiv:2501.02235](https://arxiv.org/pdf/2501.02235).


## 1. annotate.py â€“ Annotation interactive et gÃ©nÃ©ration de donnÃ©es

```mermaid
flowchart TD
  A["Chargement du document"] --> B["OCR combinÃ©<br/>(Tesseract sur plusieurs blocs de l'image trouvÃ©s intelligemment)"]
  B --> C["Interface web Flask<br/>pour sÃ©lectionner des bboxes"]
  C --> D["SÃ©lection manuelle des bbox<br/>et choix de labels"]
  D --> E["Enregistrement des annotations<br/>temp_annot.jsonl"]
  E --> F["GÃ©nÃ©ration automatique dâ€™augmentations<br/>(contraste, bruit, rotationâ€¦)"]
  F --> G["Re-OCR sur images augmentÃ©es"]
  G --> H["RÃ©-ajustement des bbox et sauvegarde JSONL"]
```

Dans **annotate.py**, on propose une application Flask permettant dâ€™annoter **manuellement** des donnÃ©es visuelles en **sÃ©lectionnant** directement les bounding boxes dÃ©tectÃ©es par OCR et en choisissant **librement** les labels (totalement customisables) :contentReference[oaicite:0]{index=0}:contentReference[oaicite:1]{index=1}.

- **But principal** : crÃ©er un fichier `temp_annot.jsonl` qui servira au **finetuning** du modÃ¨le KIE (LayoutLMv3 ou Ã©quivalent).  
- **OCR multicouche** : trois moteurs (EasyOCR, Tesseract, Docling) sont appliquÃ©s sÃ©quentiellement puis combinÃ©s sans chevauchement pour obtenir la reconstruction la plus **prÃ©cise** possible des mots et de leurs boÃ®tes :contentReference[oaicite:2]{index=2}:contentReference[oaicite:3]{index=3}.   
- **Annotation manuelle** :  
  - Lâ€™utilisateur sÃ©lectionne dans lâ€™interface **toutes** les bounding boxes quâ€™il souhaite annoter, puis entre le label de son choix (100 % customisable).  
  - Le systÃ¨me gÃ©nÃ¨re automatiquement des **tags BIO** :  
    - `B-<LABEL>` pour le premier bbox dâ€™une entitÃ©  
    - `I-<LABEL>` pour chaque bbox suivant  
  - **Exemple** : lâ€™OCR a fragmentÃ© le nom `Monsieur Patate LTD` en trois bboxes.  
    1. SÃ©lectionner les trois  
    2. Choisir le label `NAME`  
    3. Stockage dans `temp_annot.jsonl` :  
    ```json
    [
      { "bbox": [[x1, y1, x2, y2], [x3, y3, x4, y4], [x5, y5, x6, y6]], "words": ["Monsieur" , "Patate" "LTD", "lives" "in", "Toulouse"], "label": ["B-NAME", "I-NAME", "I-NAME", "O", "O", "B-City"]}
    ]
    ```  
  - Si une mÃªme entitÃ© est Ã©clatÃ©e en plusieurs morceaux par lâ€™OCR, il suffit de sÃ©lectionner **tous** les morceaux pour quâ€™ils soient Ã©tiquetÃ©s ensemble (l'un aprÃ¨s l'autre, avant de valider la labÃ©lisation); lâ€™utilisateur clique-glisse pour sÃ©lectionner **toutes** les boxes formant une entitÃ© (mÃªme si lâ€™OCR lâ€™a dÃ©coupÃ©e en plusieurs morceaux) et lui associe un label. Le systÃ¨me gÃ©nÃ¨re automatiquement des tags **BIO** (â€B-â€ pour le dÃ©but, â€I-â€ pour la suite).  
   - Exemple : pour un nom en 3 boxes (Â« Monsieur Â» Â« Patate Â» Â« LTD Â»), on sÃ©lectionne les trois, on choisit le label `NAME` et on obtient dans `temp_annot.jsonl` :  
     ```json
     ["B-NAME", "I-NAME", "I-NAME"]
     ```  
  - Note que le label "O" sert de "N/A" (non-applicable). Ce sont les mots du documents qu'on n'a pas labÃ©lisÃ©s. Eh oui, parce que la tÃ¢che de KIE sur des documents est en fait une tÃ¢che de token classification, cad qu'il faut classifier CHAQUE token, mÃªme si tous les tokens ne nous intÃ©ressent pas!   
- **Augmentation et rÃ©ajustement** : une fois les annotations sauvegardÃ©es dans `temp_annot.jsonl` (via le bouton **Save to Annotation File**), annotate.py applique des transformations dâ€™image (contraste, bruit, rotation lÃ©gÃ¨re, perspective, taches dâ€™encre, etc.), relance lâ€™OCR sur chaque image modifiÃ©e, et **rÃ©ajuste** les annotations par transfert de labels basÃ© sur la similaritÃ© de texte :contentReference[oaicite:4]{index=4}:contentReference[oaicite:5]{index=5}.  
  1. Application de transformations dâ€™image (contraste, bruit, rotation, perspective, tachesâ€¦)  
  2. Nouvelle passe OCR sur chaque image modifiÃ©e  
  3. Transfert automatique des labels : rapprochement des textes pour **rÃ©ajuster** les bboxes  
- **Sortie finale** : le fichier `temp_annot.jsonl`, normalisÃ© pour LayoutLMv3 (bboxes Ã  lâ€™Ã©chelle 0â€“1000), servira ensuite de jeu de donnÃ©es pour le **finetuning** du modÃ¨le KIE.

---


## 2. Fine-tuning du modÃ¨le multimodal (LayoutLMv3)
- **Lancer le script** :
```shell
python layoutlmv3_ft/layoutlmv3_ft.py
```
- Chargement du backbone :
    - Tokens textuels issus de lâ€™OCR
    - BoÃ®tes englobantes (bboxes normalisÃ©es)
    - Features visuelles extraites de lâ€™image
- TÃªte spÃ©cialisÃ©e : classification de token pour extraire les paires clÃ©-valeur
- HyperparamÃ¨tres : configurables via configs/layoutlmv3_config.yaml (learning rate, batch size, epochs)
- MatÃ©riel :
    - CPU (multi-threading) ou petite GPU (CUDA minimale)
    - DurÃ©e typique : ~2â€“4Â h sur CPU, <1Â h sur GPU modeste
- Sortie : Poids sauvegardÃ©s dans results/final_model/model.safetensors

---

## 3. inference.py â€“ Chargement du modÃ¨le fine-tuned et prÃ©dictions

Le script **inference.py** ne fonctionne **quâ€™aprÃ¨s** le fine-tuning lancÃ© dans le dossier `layoutlmv3_ft` (aprÃ¨s clic sur **Finish and clean** dans annotate.py, on exÃ©cute le notebook/train script qui produit `results/final_model`) :contentReference[oaicite:6]{index=6}:contentReference[oaicite:7]{index=7}.  
1. **Chargement du modÃ¨le** : on rÃ©cupÃ¨re `label_mappings.json` et le modÃ¨le LayoutLMv3ForTokenClassification entraÃ®nÃ©, via `LayoutLMv3Processor` (avec OCR dÃ©sactivÃ©), sur le device CPU/GPU disponible :contentReference[oaicite:8]{index=8}:contentReference[oaicite:9]{index=9}.  
2. **Pipeline de prÃ©diction** :  
   - TÃ©lÃ©versement dâ€™une image via lâ€™interface Flask.  
   - **OCR combinÃ©** : Docling dâ€™abord, puis Tesseract pour ajouter les mots manquants.  
   - **Normalisation** des boÃ®tes au format 0â€“1000 attendu par LayoutLMv3.  
   - **InfÃ©rence** : passage dans le modÃ¨le pour obtenir un label par token, puis agrÃ©gation des premiers tokens de chaque mot.  
3. **Post-traitement** :  
   - **Fusion BIO** : on regroupe sÃ©quentiellement les a

---


## Licence

Ce projet est distribuÃ© sous licence Apache 2.0. Voir le fichier [LICENSE](./LICENSE) pour les dÃ©tails.